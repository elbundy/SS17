\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{esvect}
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{graphicx}


\DeclareMathOperator*{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{argmax} % no space, limits underneath in displays

\title{Pattern Analysis - Lecture notes}
\author{Sebastian Rietsch}

\begin{document}
\maketitle
\section{Density Estimation}
Let \(p(\vec{x})\) denote a probability density function (pdf) then:
\begin{enumerate}
    \item
        \(p(\vec{x}) \geq 0 \)
    \item
        \(\int\limits_{-\infty}^{+\infty} p(\vec{x}) d\vec{x} = 1 \)
    \item
        \(p(\vec{a} \leq \vec{x} \leq \vec{b}) = \int\limits_{\vec{a}} p(\vec{x}) d\vec{x} \)
\end{enumerate}
The task of density estimation is to obtain from a set of discrete samples/measurements a continuos representation of the underlying pdf.

\subsection{Parametric density estimation}
Make an assumption about the underlying distribution and determine the best fitting distribution parameters from the data. (MLE, Maximum a Positriori Estimation)

\subsection{Non-parametric density estimation: Parzen-Rosenblatt estimator}
The Parzen window estimator interpolates the pdf from the observations in the neighborhood of a position \(\vec{x}\), using an appropriate kernel (or window) function.

\subsubsection*{Short derivation}
Let \(p_R\) denote the probabilty that \(\vec{x}\) lies within region \(R\): \(p_R = \int_R p(\vec{x}) d\vec{x}\).\\
Now assume that \(p(\vec{x})\) is approximately constant in \(R\).
 \[\Rightarrow p_R = p(\vec{x}) \underbrace{\int_R d\vec{x}}_\text{volume of R}\]\\
For example let \(R\) be a d-dimensional hypercube with side length \(h\), then its volume is \(h^d\) and \(p_R \approx p(\vec{x}) \cdot V_R\).

Let \(p_R = \frac{k_R}{N}\), i.e. we determine the probability of making an observation in region \(R\) by counting the samples in \(R\) \((k_R)\) and dividing by the total number of samples. (\(p_R\) is also called the "relative frequency")\\
\[\Rightarrow p(\vec{x}) = \frac{p_R}{V_R} = \frac{k_R}{V_R \cdot N}\]\\
Let us write the Parzen window estimator as a function of a kernel \(k(\vec{x},\vec{x_i})\), then
\[p(\vec{x}) = \frac{1}{Nh^d} \cdot \sum\limits_{i=1}^{N} k(\vec{x}, \vec{x_i})\]
and
\[k(\vec{x}, \vec{x_i}) = \begin{cases} 
    1 & \frac{|\vec{x}_{i,k} - \vec{x}_k|}{h} \leq \frac{1}{2} \\ 0 & \text{otherwise}
\end{cases}\].\\
In any dimension \(k\),  \(\vec{x_i}\) and \(\vec{x}\) are not farther apart than \(0.5\cdot h\).

Equivalently, if we use a (multivatiate) Gaussian kernel:
\[ k(\vec{x}, \vec{x_i}) = \underbrace{\frac{1}{(2\pi)^d |\Sigma|}}_{\text{Volume of Gaussian}} e^{-(\vec{x} - \vec{x_i})^T \Sigma^{-1} (\vec{x} - \vec{x_1})} \;\; \Rightarrow \;\; p(\vec{x}) = \frac{1}{N} \sum\limits_{i=1}^{N} k(\vec{x}, \vec{x_i})\]

\subsubsection*{A note on application}
\begin{itemize}
    \item
        General remark: we obtain a continous pdf, i.e. density estimation converts a list of measurements to a statistical model
    \item
        One specific example: we can sample from a pdf. This means that we have a principled way of generating new/more ... data that behaves/looks similarly to the observations
\end{itemize}

\subsubsection*{Question: How can we (practically) sample from a pdf?}
\begin{enumerate}
    \item
        Convert pdf into cumulative density function (cdf) (\( cdf[i] = cdf[i-1] + pdf[i]\))
    \item
        Draw a uniformly distributed number (\(r\)) between 0 and 1
    \item
        Our sampled value is \(x\), where \(cdf[x] = r\)  
\end{enumerate}

\subsubsection*{How can we determine good window size?}
Let's do a Maximum Likelihood with cross validation. e.g. leave-one-sample-out cross-validation (CV).
\[ p_{h, N-1}^j (\vec{x}) = \frac{1}{N h^d} \sum_{i=1, i \neq j}^{N} k(\vec{x}, \vec{x_i}) \] \\
We estimate the pdf from all samples except \(\vec{x_j}\). \(\vec{x_j}\) will be used to evaluate the quality of the pdf using window size h.
\begin{align*}
\hat{h} &= \argmax_h \mathcal{L}(h) = \argmax_h \prod_{j=1}^{N} p_{h, N-1}^j (\vec{x_j})\\
&= \argmax_h \sum\limits_{j=1}^{N} log \; p_{h, N-1}^j (\vec{x_j})
\end{align*}
The position of the maximum (when using log-likelihood) does not change, because the logarithm is a strictily monotonic function.

\section{Mean Shift Algorithm (Comaniciu, Meer)}
\textbf{Purpose}: Find maxima in a pdf without actually performing a full density estimation (e.g. for Clustering (maximum is cluster center), segmentation, ...)

Assume that we have a full density estimator. Then
\[p(\vec{x}) = \frac{1}{N} \sum\limits_{i=1}^N k_h (\vec{x}, \vec{x_i}) \].\\
\textbf{Idea}: Maxima can be found where the gradient of the pdf is zero.\\
\textbf{Problems}:
\begin{itemize}
    \item
        It may be the case, that a zero gradient is just between two finer maxima
    \item
        The kernel size indirectly controls the number of identified maxima
\end{itemize}
... TODO: lectures 3-5

\section{Manifold Learning}
Curse of Dimensionality: "Human intuition breaks down in highdimensional spaces", Bellman 1961

Let us illustrate this: consider d-dimensional feature vectors $\vec{x_1}, \vec{x_2}, \dots \vec{x_N} \in \mathbb{R}^d$ where $0 \leq x_{i, k} \leq 1$ uniformly distributed in a $d$-dimensional hypercube of volume $1$.

Let's say we would like to cover enough volume of that cube to collect $1\%$ of the data. Let's say we also use a cube for this task. What is the required edge-length of the cube to obtain $1\%$ of space? For example, for a 10-dimensional hypercube:
\[V = s^d \Rightarrow s = V^{\frac{1}{d}} = (0.01)^{\frac{1}{10}} = 0.63\]
Less surprising perspective: consider to draw a $10$-dim vector, but in no dimension exceed $0.63$ (very improbable).
Another way of thinking about this is that in a very high dimensional space, virutally every feature point is located at the boundary of the space (because in at least one dimension we draw a very low or very high value).

This leads to the effect that common distance measures loose their effectivity. This can be seen by looking at the \textbf{median} distance of the nearest neighbor to the origin, given $N$ samples in $d$ dimensions: $dist(d, N) = (1 - \frac{1}{2}^{\frac{1}{N}})^{\frac{1}{d}}$\\
$\Rightarrow N=5000, d=10: dist(10, 5000) = 0.52$
(TODO: lectures 7-9)

\section{Hidden Markov Models}
\textbf{Motivation:} model dependencies between features in a sequence. A Hidden Markov Model (HMM) is a generative probabilistic approach to describing sequential data. (Example: speech data)

Let \(S_1, \dots, S_{N}\) denote $N$ hidden states, and let \(o_1, \dots, o_M\) denote \(M\) features/observations that form a sequence \(\langle o_1, \dots, o_M \rangle\) (order matters). A HMM models the joint probability of a sequence of hidden states and a sequence of observations \(p(\langle o_1, \dots, o_M \rangle, \langle S_1, \dots, S_M \rangle)\). This joint probability makes the HMM a generative approach.

The model consists of state transition probabilities \(a_{ij} = P[q_{t+1} = S_j | q_t = S_i]\), a PDF per state that describes the probability to make a specific observation: \( b_j (v_k) = P[ v_k \text{ at state } S_j]\) where $V = \{v_1, \dots, v_{N_S}\}$ is the set of possible observations and a probability \(\pi_i = P[ q_1 = S_i] \) that the first/starting state is \(S_i\).

We commonly subsume these parameters as \(\lambda = (A, B, \vec{\pi}) \). (\( A: (N \times N), B: (M \times N), \pi: (N \times 1)\)).

\includegraphics[scale=0.4]{hmm.jpg}

\noindent Questions that we would like to answer:
\begin{enumerate}
    \item
        Given a model $\lambda$, what is \(P(\langle o_1, \dots, o_M \rangle | \lambda) \), the probability of making an observation $\langle o_1, \dots, o_N \rangle$ given $\lambda$?
    \item
        Given a model $\lambda$ and a an observation sequence \(\langle o_1, \dots, o_M \rangle \), what is the most likely (= highest probability) sequence of states \( \langle S_1, \dots, S_M \rangle \)?
    \item
        How can we obtain the model parameters $\lambda$ in a fully automated way (training)?
\end{enumerate}

\noindent On question 1: 

\( p(\langle o_1, \dots, o_M \rangle, \langle S_1, \dots, S_M \rangle) = p(\langle S_1, \dots, S_M \rangle) \cdot p(\langle o_1, \dots, o_M \rangle | \langle S_1, \dots, S_M \rangle) \)

Assuming that the state sequence \(\langle S_1, \dots, S_M \rangle\) is known:

%\(\underbrace{ = \pi_{S_1} \cdot a_{S_1, S_2} \cdot a_{S_2, S_3} \cdot \dots \cdot a_{S_{M-1}, S_M \)} }%_{\(p(\langle S_1, \dots, S_M \rangle)} 

%(\( = \pi_{S_1} \) means first state in sequence, not first state)

Formel 1

Note that the state sequence is not known when we seek \(p(\langle o_1, \dots, o_M \rangle) \) (Question 1), thus we need to marginalize over all possible state sequences:

Formel 2

\end{document}
